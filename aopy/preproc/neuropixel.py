# neuropixel.py
# 
# Preprocessing neuropixel data

import os
import glob

import numpy as np
from ibldsp.voltage import destripe_lfp
from pathlib import Path
from ..utils import extract_barcodes_from_times, get_first_last_times, sync_timestamp_offline, convert_port_number
from scipy.io import savemat
from .. import data as aodata

def sync_neuropixel_ecube(raw_timestamp,on_times_np,off_times_np,on_times_ecube,off_times_ecube,inter_barcode_interval=20,bar_duration=0.02,verbose=False):
    '''
    This function is specfic to synchronization between neuropixels and ecube.
    
    Args:
        raw_timestamp (nt) : raw timestamp that is not synchronized
        on_times_np (ndarray) : timings when sync line rises to 1 in neruopixel streams
        off_times_np (ndarray): timings when sync line returns to 0 in neruopixel streams
        on_times_ecube (ndarray): timings when sync line rises to 1 in ecube streams
        off_times_ecube (ndarray): timings when sync line returns to 0 in ecube streams
        bar_duration (float): duration of each bar that is sent to each stream
        verbose (bool): print barcode times and barcodes for each stream
    
    Returns:
        tuple: tuple containing:
            | **sync_timestamps (nt):** synchronized timestamps
            | **scaling (float):** scaling factor between streams
    '''
    
    # Get each barcode timing and label
    barcode_ontimes_np,barcode_np = extract_barcodes_from_times(on_times_np,off_times_np,inter_barcode_interval=inter_barcode_interval,bar_duration=bar_duration)
    barcode_ontimes_ecube,barcode_ecube = extract_barcodes_from_times(on_times_ecube,off_times_ecube,inter_barcode_interval=inter_barcode_interval,bar_duration=bar_duration)
        
    # Get the first and last barcode timing in the recording at each stream
    first_last_times_np, first_last_times_ecube = get_first_last_times(barcode_ontimes_np,barcode_ontimes_ecube,barcode_np, barcode_ecube) 

    if verbose:
        print(f'neuropixels barcode times: {first_last_times_np}, ecube barcode times{first_last_times_ecube}')
        
    return sync_timestamp_offline(raw_timestamp, first_last_times_np, first_last_times_ecube)

def classify_ks_unit(spike_times, spike_label):
    '''
    Classify unit activity identified by kilosort into each single unit
    
    Args:
        spike_times (nspikes): spike times generated by kilosort
        spike_label (nspikes): cluster labels of each spike generated by kilosort
        
    Returns:
        spike_times_unit (dict): spike times for each unit
    '''
    
    spike_times_unit = {}
    
    for unit_label in np.unique(spike_label):
        spike_times_unit[f'{unit_label}'] = spike_times[spike_label==unit_label.astype(int)]
    
    return spike_times_unit

def parse_ksdata_entries(kilosort_dir, concat_data_dir):
    '''
    Parse concatenated data processed by kilosort into the task entries and save relevant data (spike_indices, spike_clusters, and ks_label)
    
    Args:
        kilosort_dir (str): kilosort directory (ex. '/data/preprocessed/kilosort')
        concat_data_dir (str): data directory that contains concatenated data and kilosort_output (ex. '2023-06-30_Neuropixel_ks_affi_bottom_port1')
        
    Returns
        None
    '''
    
    # Load kilosort data
    kilosort_output = aodata.load_ks_output(kilosort_dir, concat_data_dir, flag='spike')
    spike_indices = kilosort_output['spike_indices']
    spike_clusters = kilosort_output['spike_clusters']
    ks_label = kilosort_output['ks_label']
    
    # Load datasize of each entry and filename
    data_path = os.path.join(kilosort_dir, concat_data_dir)
    datasize_entry = np.load(os.path.join(data_path, 'datasize_entry.npy'))
    task_paths = np.load(os.path.join(data_path, 'task_path.npy'))

    nentry = datasize_entry.shape[0]

    datasize_entry = np.cumsum(datasize_entry)
    for idx in range(nentry):
        # Get spike indices included in a corresponding entry
        if idx == 0:
            ientry_idx = spike_indices < datasize_entry[idx]
        else:
            ientry_idx = (spike_indices >= datasize_entry[idx-1])&(spike_indices < datasize_entry[idx])

        # Subtract previous entries' datasize so that the first index of each entry can become 0.
        spike_indices_entry = spike_indices[ientry_idx]
        if idx > 0:
            spike_indices_entry -= datasize_entry[idx-1]
        
        # Make new directory to save parsed data
        taskid = task_paths[idx].split('_')[-1][2:]
        task_save_dir = f'{concat_data_dir}_{taskid}'
        task_save_path = os.path.join(kilosort_dir, task_save_dir)
        if not os.path.exists(task_save_path):
            os.makedirs(task_save_path)
        
        # save data
        np.save(os.path.join(task_save_path, 'spike_indices_entry'), spike_indices_entry)
        np.save(os.path.join(task_save_path, 'spike_clusters_entry'), spike_clusters[ientry_idx])
        np.save(os.path.join(task_save_path, 'ks_label'), ks_label.astype('str'))
        
def concat_neuropixel(np_datadir, kilosort_dir, subject, te_ids, date, port_number, max_memory_gb = 0.1):
    '''
    Concatenate continuous.dat files of different sessions specified by multiple task ids (te_ids)
    The concatenated data is saved into a port folder of the kilosort preproc directory
    
    Args:
        np_datadir (str): neuropixel data directory where the data files are located (ex. '/data/raw/neuropixels')
        kilosort_dir (str): kilosort directory (ex. '/data/preprocessed/kilosort')
        subject (str): subject name
        te_ids (list): list of task ids to concatenate
        date (str): date of recording (ex. '2023-04-14')
        port_number (int): port number which a probe connected to. natural number from 1 to 4.
        max_memory_gb (float): max memory used to load binary data at one time
        
    Returns:
        None
    '''

    # Extract directory names from task ids
    concat_path = []
    for task_id in te_ids:
        task_path = f'{date}_Neuropixel_{subject}_te{task_id}'
    concat_path.append(task_path)

    assert len(concat_path) > 0, "No Data to Concatenate"

    dtype = 'int16'
    for idx_t, task_group in enumerate(concat_path):
        savedir_name = f'{date}_Neuropixel_{subject}_concat{idx_t+1}' # Directory name to save concatenated data
        savedir_path = Path(kilosort_dir) / savedir_name / f'port{port_number}'

        if not os.path.exists(savedir_path):
            os.makedirs(savedir_path)

        print('\n', 'Working in ', savedir_path)

        datasize_entry = []
        file_path = []
        for idx, np_recorddir in enumerate(task_group):
            print(f'Processing {np_recorddir}')

            _,metadata = aodata.load_neuropixel_data(np_datadir, np_recorddir, 'ap', port_number = port_number)
            nch = metadata['num_channels']

            probe_dir = convert_port_number(port_number)
            data_path = os.path.join(np_datadir, np_recorddir)
            continuous_data_path = glob.glob(os.path.join(data_path,f'**/*{probe_dir}/continuous.dat'),recursive=True)[0]

            data = np.memmap(continuous_data_path, dtype = dtype)

            file_path.append(np_recorddir)
            datasize_entry.append(int(data.shape[0]/nch))
  
            # Save data (concatentate data)
            chunksize = int(max_memory_gb * 1e9 / np.dtype(dtype).itemsize / nch)
            nchunk = int(np.ceil(data.shape[0]/chunksize))
                
            for ichunk in range(nchunk):
                if ichunk != nchunk-1:                    
                    data_reshape = data[ichunk*chunksize*nch:(ichunk+1)*chunksize*nch].reshape(-1,nch)
                else:
                    data_reshape = data[ichunk*chunksize*nch:].reshape(-1,nch)
                        
                if (idx == 0) & (ichunk == 0):
                    save_filename = 'continuous.dat'
                    f = open(os.path.join(savedir_path,save_filename), 'wb') # save
                    data_reshape.tofile(f)
                    f.close()
                else:
                    f = open(os.path.join(savedir_path,save_filename), 'a') # append
                    data_reshape.tofile(f)
                    f.close()

        # Save channel position as mat file for kilosort
        pos = {'xpos': metadata['xpos'],'ypos':metadata['ypos']}
        savemat(os.path.join(savedir_path, 'channel_pos.mat'), pos)
                          
        # Save datasize and filename of each entry to parse data after spike sorting
        np.save(os.path.join(savedir_path, 'datasize_entry'), np.array(datasize_entry))
        np.save(os.path.join(savedir_path, 'task_path'), np.array(file_path))

def sync_ts_data_timestamps(data, sync_timestamps):
    '''
    Synchronize time series data by padding nan or cropping datapoints based on synchronized timestamps
    so that data would begin at 0 on the synchronized time axis
    
    Args:
        data (nt, nch): time series data to preprocess
        sync_timestamp (nt): synchronized timestamps
        
    Returns:
        sync_data (sync_nt, nch): synchronized time series data. The shape of data changes.
    '''
    
    dt = sync_timestamps[1]-sync_timestamps[0]
    
    if data.ndim == 1:
        data = data[:,np.newaxis]
        nch = data.shape[1]

    # When the initial timestamp is more than 0, pad np.nan so that data could begin at time 0
    if sync_timestamps[0]>=0:
        tmp = np.arange(sync_timestamps[0]-dt,0,-dt)
        padding_datapoints = tmp.shape[0]
        pad_to_data = np.zeros((padding_datapoints,nch))*np.nan
        sync_data = np.concatenate([pad_to_data, data],axis=0)
        sync_timestamps = np.concatenate([pad_to_data[:,0], sync_timestamps],axis=0)
        
    # When the initial timestamp is less than 0, crop the head of data so that data could begin at time 0
    else:
        not_crop_datapoints = sync_timestamps >= 0
        sync_data = data[not_crop_datapoints,:]
        sync_timestamps = sync_timestamps[not_crop_datapoints]
        
    return np.squeeze(sync_data), sync_timestamps

def destripe_lfp_batch(lfp_data, save_path, sample_rate, bit_volts, max_memory_gb = 1., dtype='int16', min_batch_size = 21):
    
    '''
    Destripe LFP data in each batch to save memory. The result is saved in save_path.
    
    Args:
        lfp_data (nt, nch): LFP data. This should be a memory mapping array.
        save_path (str): file path to save destriped lfp data
        sample_rate (float): sampling rate in Hz
        bit_volts (float): volt per bit
        max_memory_gb (float): memory size in GB to determine batch size. default is 1.0 GB.
        dtype (str, optional): dtype for data. default is int16.
        min_batch_size (int): the number of size in integer to ensure that batch size is more than min_batch_size to run destripe_lfp
    
    Returns:
        None
    
    '''

    # Load data and metadata
    n_samples, n_channels = lfp_data.shape

    # Create memmap array to save destriped lfp data
    lfp_destriped = np.memmap(save_path, dtype=dtype, mode='w+', shape=lfp_data.shape)

    # Destripe lfp
    batch_size = int (max_memory_gb*1e9 / (n_channels*np.dtype(type(bit_volts)).itemsize))
    batch_size += min_batch_size # ensure that batch size is more than 21 to run destrip_lfp
    Nbatches = np.ceil(n_samples/batch_size).astype(int)

    for ibatch in range(Nbatches):
        tmp = destripe_lfp((lfp_data[ibatch*batch_size:(ibatch+1)*batch_size, :]*bit_volts).T/1e6, sample_rate)*1e6
        lfp_destriped[ibatch*batch_size:(ibatch+1)*batch_size, :] = (tmp/bit_volts).T.astype(dtype)
        lfp_destriped.flush()