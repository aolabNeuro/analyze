import numpy as np
from ..utils import extract_barcodes_from_times, get_first_last_times, sync_timestamp_offline, convert_port_number
from ..data import load_neuropixel_data, load_ks_output
import os
import glob

def sync_neuropixel_ecube(raw_timestamp,on_times_np,off_times_np,on_times_ecube,off_times_ecube,bar_duration=0.02,verbose=False):
    '''
    This function is specfic to synchronization between neuropixels and ecube.
    
    Args:
    raw_timestamp (nt) : raw timestamp that is not synchronized
    on_times_np (ndarray) : timings when sync line rises to 1 in neruopixel streams
    off_times_np (ndarray): timings when sync line returns to 0 in neruopixel streams
    on_times_ecube (ndarray): timings when sync line rises to 1 in ecube streams
    off_times_ecube (ndarray): timings when sync line returns to 0 in ecube streams
    bar_duration (float): duration of each bar that is sent to each stream
    verbose (bool): print barcode times and barcodes for each stream
    
    Returns:
        tuple: tuple containing:
            |** sync_timestamps (nt):** synchronized timestamps
            |** scaling (float):** scaling factor between streams
    '''
    
    while bar_duration >0.001:
        # Get each barcode timing and label
        barcode_ontimes_np,barcode_np = extract_barcodes_from_times(on_times_np,off_times_np,bar_duration=bar_duration)
        barcode_ontimes_ecube,barcode_ecube = extract_barcodes_from_times(on_times_ecube,off_times_ecube,bar_duration=bar_duration)

        # Check if barcodes are consistent across streams
        n_barcode = min(len(barcode_ecube),len(barcode_np))
        num_different_barcodes = 0
        for idx in range(n_barcode):
            barcode = barcode_ecube[idx]
            if barcode != barcode_np[idx]:
                num_different_barcodes += 1
        
        # If barcodes are the same across streams, break this loop
        if num_different_barcodes == 0:
            break
        bar_duration -= 0.0001
            
    if verbose:
        print(f'bar duration: {bar_duration}\n')
        print(f'neuropixel barcode times: {barcode_ontimes_np}\n')
        print(f'neuropixel barcode: {barcode_np}\n')
        print(f'ecube barcode times: {barcode_ontimes_ecube}\n')
        print(f'ecube barcode: {barcode_ecube}\n')
        
    # Get the first and last barcode timing in the recording at each stream
    first_last_times_np, first_last_times_ecube = get_first_last_times(barcode_ontimes_np,barcode_ontimes_ecube,barcode_np, barcode_ecube) 

    return sync_timestamp_offline(raw_timestamp, first_last_times_np, first_last_times_ecube)

def classify_ks_unit(spike_times, spike_label):
    '''
    Classify unit activity identified by kilosort into each single unit
    
    Args:
        spike_times (nspikes): spike times generated by kilosort
        spike_label (nspikes): Cluster labels of each spike generated by kilosort
        
    Returns:
        spike_times_unit (dict): spike times for each unit
    '''
    
    spike_times_unit = {}
    
    for unit_label in np.unique(spike_label):
        spike_times_unit[f'{unit_label}'] = spike_times[spike_label==unit_label.astype(int)]
    
    return spike_times_unit

def parse_ksdata_entries(preproc_dir, subject, date, port_number=1):
    '''
    Parse concatenated data processed by kilosort into the task entries and save relevant data (spike_indices and spike_clusters)
    
    Args:
        preproc_dir (str): preprocessed directory where the files live
        subject (str): subject name
        date (str): date (str): date of recording (ex. '2023-04-14')
        node_idx (int, optional): record node index. This is usually 0
        ex_idx (int, optional): experiment index. This is usually 0
        port_number (int, optional): port number which a probe connected to. natural number from 1 to 4.
        
    Returns
        None
    '''
    # Load kilosort data
    kilosort_output = load_ks_output(preproc_dir, subject, date, port_number=port_number, flag='spike')
    spike_indices = kilosort_output['spike_indices']
    spike_clusters = kilosort_output['spike_clusters']

    # Find path for a task entry corresponding to experiment day
    probe_name = convert_port_number(port_number)
    kilosort_path = os.path.join(preproc_dir, f'spike_sorting/{date}_Neuropixel_{subject}_kilosort')
    data_path = os.path.join(kilosort_path,probe_name)
    
    # Load datasize of each entry and filename
    datasize_entry = np.load(os.path.join(data_path, 'datasize_entry.npy'))
    task_paths = np.load(os.path.join(data_path, 'task_path.npy'))

    nentry = datasize_entry.shape[0]

    datasize_entry = np.cumsum(datasize_entry)
    for idx in range(nentry):
        # Get spike indices included in a corresponding entry
        if idx == 0:
            ientry_idx = spike_indices < datasize_entry[idx]
        else:
            ientry_idx = (spike_indices >= datasize_entry[idx-1])&(spike_indices < datasize_entry[idx])

        # Subtract previous entries' datasize so that the first index of each entry can become 0.
        spike_indices_entry = spike_indices[ientry_idx]
        if idx > 0:
            spike_indices_entry -= datasize_entry[idx-1]
        
        # Make new directory to save parsed data
        taskid = task_paths[idx].split('_')[-1][2:]
        task_save_path = os.path.join(preproc_dir, f'spike_sorting/kilosort_{date}_{subject}_{taskid}_np')
        if not os.path.exists(task_save_path):
            os.makedirs(task_save_path)
            
        probe_save_path = os.path.join(task_save_path, probe_name)
        if not os.path.exists(probe_save_path):
            os.makedirs(probe_save_path)
        
        # save data
        np.save(os.path.join(probe_save_path, 'spike_indices_entry'), spike_indices_entry)
        np.save(os.path.join(probe_save_path, 'spike_clusters_entry'), spike_clusters[ientry_idx])
        
def concat_neuropixel_within_day(np_datadir, subject, date, preproc_dir, port_number = 1):
    '''
    concatenate continuous.dat files of different sessions within day and monkey for spike sorting
    then save the concatenated continuous data into preproc directory with datasize and task id for each task entry.
    The datasize and task id are used to devide spikes into spikes in each task entry later.
    
    Args:
        np_datadir (str): neuropixel data directory where the data files are located
        subject (str): subject name
        date (str): date (str): date of recording (ex. '2023-04-14')
        preproc_dir (str): preprocessed directory ('ex. /data/preprocessed')
        port_number (int, optional): port number which a probe connected to. natural number from 1 to 4.
        
    Returns:
        None
    '''

    # Make a new kilosort directory to save data in preproc directory
    kilosort_path = os.path.join(preproc_dir, f'spike_sorting/{date}_Neuropixel_{subject}_kilosort')
    if not os.path.exists(kilosort_path):
        os.makedirs(kilosort_path)

    # Make a new probe directory to save a different probe data separately in the kilosort directory
    probe_dir = convert_port_number(port_number)
    savedir_path = os.path.join(kilosort_path, probe_dir)
    if not os.path.exists(savedir_path):
        os.makedirs(savedir_path)

    # Find path for task entries within day and subject
    data_path = np.array(glob.glob(os.path.join(np_datadir,f'{date}_Neuropixel_{subject}_te*'),recursive=True))
    data_path = np.sort(data_path) # sort in the order of task_id

    # Load continuous data at each task entry
    datasize_entry = []
    file_path = []
    for idx, path in enumerate(data_path):
        # Find data path for continuous.dat recorded by the same probe
        continuous_data_path = glob.glob(os.path.join(path,f'**/*{probe_dir}/continuous.dat'),recursive=True)[0]

        # Load continuous data as numpy.memmap
        np_recorddir = os.path.split(path)[1]
        _, metadata = load_neuropixel_data(np_datadir,np_recorddir,'ap')
        data = np.memmap(continuous_data_path, dtype='int16')
        data_reshape = data.reshape(-1,metadata['num_channels'])
        
        file_path.append(np_recorddir)
        datasize_entry.append(data_reshape.shape[0])

        # Save data
        if idx == 0:
            save_filename = 'continuous.dat'
            f = open(os.path.join(savedir_path,save_filename), 'wb') # save
            data_reshape.tofile(f)
            f.close()
        else:
            f = open(os.path.join(savedir_path,save_filename), 'a') # append
            data_reshape.tofile(f)
            f.close()

    # Save datasize and filename of each entry for preprocessing after spike sorting
    np.save(os.path.join(savedir_path, 'datasize_entry'), np.array(datasize_entry))
    np.save(os.path.join(savedir_path, 'task_path'), np.array(file_path))